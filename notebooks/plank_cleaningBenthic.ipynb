{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a database of the water in the Chesapeake Bay.\n",
    "\n",
    "The CSV from the API (combined with station data) has the columns \n",
    "`CBSeg2003`, `CBSeg2003Description`, `Station`, `Latitude`, `Longitude`,\n",
    "`FieldActivityId`, `Source`, `SampleType`, `SampleDate`, `Layer`,\n",
    "`SampleNumber`, `GMethod`, `TSN`, `LatinName`, `Size`, `Method`,\n",
    "`Parameter`, `ReportingValue`, `ReportingUnit`, `NODCCode`, `SPECCode`,\n",
    "`SerialNumber`\n",
    "\n",
    "Here is a descriptor of the columns, from [The 2012 Users Guide to CBP Biological Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/guide2012_final.pdf):\n",
    "- `CBSeg2003` 2003 Chesapeake Bay Segment Designation. Divided into regions based on circulation and salinity properties. We used 8 from the Bay proper, 2 adjoining Bays, and 1 adjoining sound.\n",
    "- `CBSeg2003Description` 2003 Chesapeake Bay Segment Designation Description in the format Location-Salinity. The locations are Chesapeake Bay, Eastern Bay, Mobjack Bay, and Tangier Sound. The salinity levels are tidal fresh (0.0 - 0.5 parts per thousand),\n",
    "oligohaline (0.5 - 5.0 parts per thousand), mesohaline (5.0 - 18.0 parts per thousand), and polyhaline (greater than 18.0 parts per thousand). \n",
    "- `Station` the sampling station\n",
    "- `Latitude` and  `Longitude`, the Latitude and Longitude for the sampling station\n",
    "- 'FieldActivityId' is not included in the database user guide\n",
    "- `Source` Data Collection Agency\n",
    "- `SampleType`  Collection Type. However, in this dataset all are C, composite sample, made up of subsamples from multiple depths.\n",
    "- `SampleDate` Sampling date (MM/DD/YYYY). We downloaded 8/9/2004 through 12/9/2021\n",
    "- `Layer` Layer of Water Column in Which Sample Was Taken. In this dataset\n",
    "     - S, Surface\n",
    "     - AP, Above pycnocline\n",
    "     - WC, Whole water column\n",
    "- `SampleNumber` number assigned to the sample\n",
    "- `GMethod` Chesapeake Bay Program Gear Method Code. Codes represent information relating to the type of field gear used to collect samples for all analysis. In this dataset all are 7, sediment Pump\n",
    "- `TSN` ITIS Taxon Serial Number, unique to the species. When used in conjunction with the NODC, the TSN\n",
    "overcomes the problem of numeric changes in the NODC code whenever species are reclassified. \n",
    "- `LatinName` Species Latin Name \n",
    "- `Size` Cell Size Groupings when taken. Some species have different measurements for different sizes. \n",
    "- `Method` Chesapeake Bay Program Sample Analysis Code. In January of 2005 in Maryland and October 2005 in Virginia, the following enumeration technique was instituted for all Chesapeake Bay Program supported phytoplankton enumerations. In this sample, the codes are\n",
    "     - PH101, MSU/ANS Phytoplankton Enumeration Method\n",
    "     - PH102, ODU Phytoplankton Enumeration Method\n",
    "     - PH102M, ODU Phytoplankton Enumeration Method-2005 Modification\n",
    "     - PH103, Uniform Chesapeake Bay Program Phytoplankton Enumeration Method\n",
    "     - PP101, ODU Picoplankton Enumeration Method\n",
    "     - PP102, MSU/ANS Picoplankton Enumeration Method \n",
    "- `Parameter` Sampling Parameter. In this dataset, all are COUNT,  the number of cells per liter\n",
    "- `ReportingValue` the value of the count\n",
    "- `ReportingUnit` This parameter describes the units in which a substance is measured. In this dataset, all are L.\n",
    "- `NODCCode` National Oceanographic Data Center Species Code. All species on the list have been assigned at least partial National Oceanographic Data Center (NODC).\n",
    "- `SPECCode` Many of the agencies reporting data containing species information have developed their own in-house species codes. All of these codes are found in the SPECCODE column of a given data type. Codes will\n",
    "vary by agency and data type. The agency code column in most cases has been given the agency name\n",
    "code in the data documentation. \n",
    "- `SerialNumber` Sample serial number. However, multiple dates and locations have the same serial number.\n",
    "\n",
    "Since there are more unique Latin names than TSN (563 vs 519), we will use Latin. There are many missing NODC Codes and SPEC Codes. In theory, these four columns all encode the same data.\n",
    "\n",
    "The main thing we will need to do is create a column for each of the 563 unique Latin names. Then put the count in the correct row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sediment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioMass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26059, 15) Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'FieldActivityId', 'BiologicalEventId', 'Source', 'SampleDate',\n",
      "       'SiteType', 'TotalDepth', 'SampleTime', 'SampleReplicate',\n",
      "       'IBIParameter', 'IBIValue'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "biomass = pd.read_csv('../data/plank_ChesapeakeBenthicBioMass.csv')\n",
    "\n",
    "print(biomass.shape,biomass.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dataframe size\n",
    "\n",
    "First, let's remove the excess columns:\n",
    "- `Source` is a bookkeepng value, but important for merging\n",
    "- `SiteType` is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26059, 14)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biomass_clean = biomass.drop(columns=['SiteType'])\n",
    "\n",
    "biomass_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also a few rows that do not encode information that were missed by the cleaning in the download. Let's drop the rows where `IBIParameter` is missing (from DataWrangler, these are the correct rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_clean = biomass_clean.dropna(subset=['IBIParameter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pivot the `IBIParameter` column and `ReportedValue` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to use row numbers as the index\n",
    "df_reset = biomass_clean.reset_index(drop=True)\n",
    "\n",
    "# Pivot the DataFrame while preserving non-pivoted columns\n",
    "biomass_pivoted = df_reset.pivot_table(index=df_reset.index, columns='IBIParameter', values='IBIValue', aggfunc='first')\n",
    "\n",
    "# Combine pivoted result with the original DataFrame columns not involved in the pivot\n",
    "biomass_pivoted= df_reset.drop(columns=['IBIParameter','IBIValue']).join(biomass_pivoted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25785, 138)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biomass_pivoted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine rows if the following all agree:\n",
    "`CBSeg2003`, `CBSeg2003Description`, `Station`, `Latitude`, `Longitude`,`EventId`,`Source`, `SampleReplicate`,`SampleDate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(832, 12)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_group = ['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude', 'FieldActivityId', 'BiologicalEventId','Source','SampleDate','TotalDepth', 'SampleTime', 'SampleReplicate']\n",
    "\n",
    "#check unique combinations\n",
    "#this allows us to check that we havent lost data\n",
    "unique_combinations = biomass_pivoted[columns_to_group].drop_duplicates()\n",
    "\n",
    "\n",
    "unique_combinations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSeg2003 True\n",
      "CBSeg2003Description True\n",
      "Station True\n",
      "Latitude True\n",
      "Longitude True\n",
      "FieldActivityId True\n",
      "BiologicalEventId True\n",
      "Source True\n",
      "SampleDate True\n",
      "TotalDepth True\n",
      "SampleTime True\n",
      "SampleReplicate True\n"
     ]
    }
   ],
   "source": [
    "# check we haven't lost unique values\n",
    "for col in columns_to_group:\n",
    "    print(col, unique_combinations[col].unique().size == \n",
    "          biomass_clean[col].unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame for processing\n",
    "biomass_processed = biomass_pivoted.copy()\n",
    "\n",
    "# Create a unique identifier for each group based on the columns to match\n",
    "biomass_processed['UniqueID'] = biomass_processed[columns_to_group].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "# Group by the unique identifier\n",
    "biomass_combined = biomass_processed.groupby('UniqueID', as_index=False).first()\n",
    "\n",
    "# Drop the UniqueID column and remove duplicates\n",
    "biomass_really_clean = biomass_combined.drop(columns='UniqueID').drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSeg2003 True\n",
      "CBSeg2003Description True\n",
      "Station True\n",
      "Latitude True\n",
      "Longitude True\n",
      "FieldActivityId True\n",
      "BiologicalEventId True\n",
      "Source True\n",
      "SampleDate True\n",
      "TotalDepth True\n",
      "SampleTime True\n",
      "SampleReplicate True\n"
     ]
    }
   ],
   "source": [
    "# Check we haven't lost unique non-empty values\n",
    "for col in columns_to_group:\n",
    "    # Filter out empty values\n",
    "    df_combined_nonempty = biomass_really_clean[col].dropna()\n",
    "    biomass_pivoted_nonempty = biomass_clean[col].dropna()\n",
    "    \n",
    "    # Check if the number of unique non-empty values matches\n",
    "    unique_check = df_combined_nonempty.unique().size == biomass_pivoted_nonempty.unique().size\n",
    "    \n",
    "    # Print the results\n",
    "    print(col, unique_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(832, 138)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biomass_really_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_really_clean.to_csv('../data/plank_ChesapeakeBayBioMass_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaterQuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8193, 15) Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'EventId', 'Source', 'SampleType', 'SampleDate', 'SampleDepth',\n",
      "       'SampleReplicate', 'ReportedParameter', 'ReportedValue',\n",
      "       'ReportedUnits', 'WQMethod'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "water = pd.read_csv('../data/plank_ChesapeakeBenthicWaterQuality.csv')\n",
    "\n",
    "print(water.shape,water.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dataframe size\n",
    "\n",
    "First, let's remove the excess columns:\n",
    "- `Source` is a bookkeepng value, but important for merging\n",
    "- `SampleType`, `SampleReplicate`, and `WQMethod` are the same (or empty) for every entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8193, 12)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_clean = water.drop(columns=['SampleType','SampleReplicate','WQMethod'])\n",
    "\n",
    "water_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features columns\n",
    "\n",
    "First, let's combine the parameter and it's units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8193, 11)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_clean['ReportedParameter'] = water_clean['ReportedParameter'] + ' '+ water_clean['ReportedUnits']\n",
    "\n",
    "water_clean = water_clean.drop(columns=['ReportedUnits'])\n",
    "\n",
    "water_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pivot the `ReportedParameter` column and `ReportedValue` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to use row numbers as the index\n",
    "df_reset = water_clean.reset_index(drop=True)\n",
    "\n",
    "# Pivot the DataFrame while preserving non-pivoted columns\n",
    "water_pivoted = df_reset.pivot_table(index=df_reset.index, columns='ReportedParameter', values='ReportedValue', aggfunc='first')\n",
    "\n",
    "# Combine pivoted result with the original DataFrame columns not involved in the pivot\n",
    "water_pivoted= df_reset.drop(columns=['ReportedParameter','ReportedValue']).join(water_pivoted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8193, 15) Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'EventId', 'Source', 'SampleDate', 'SampleDepth', 'DO MG/L',\n",
      "       'DO_SAT_P PCT', 'PH SU', 'SALINITY PSU', 'SPCOND UMHOS/CM',\n",
      "       'WTEMP DEG C'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(water_pivoted.shape,water_pivoted.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine rows if the following all agree:\n",
    "`CBSeg2003`, `CBSeg2003Description`, `Station`, `Latitude`, `Longitude`,`EventId`,`Source`, `SampleDate`, `SampleDepth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1542, 9)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_group = ['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude', 'EventId','Source','SampleDepth','SampleDate']\n",
    "\n",
    "#check unique combinations\n",
    "#this allows us to check that we havent lost data\n",
    "unique_combinations = water_pivoted[columns_to_group].drop_duplicates()\n",
    "\n",
    "\n",
    "unique_combinations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSeg2003 True\n",
      "CBSeg2003Description True\n",
      "Station True\n",
      "Latitude True\n",
      "Longitude True\n",
      "EventId True\n",
      "Source True\n",
      "SampleDepth True\n",
      "SampleDate True\n"
     ]
    }
   ],
   "source": [
    "# check we haven't lost unique values\n",
    "for col in columns_to_group:\n",
    "    print(col, unique_combinations[col].unique().size == \n",
    "          water_clean[col].unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame for processing\n",
    "water_processed = water_pivoted.copy()\n",
    "\n",
    "# Create a unique identifier for each group based on the columns to match\n",
    "water_processed['UniqueID'] = water_processed[columns_to_group].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "# Group by the unique identifier\n",
    "water_combined = water_processed.groupby('UniqueID', as_index=False).first()\n",
    "\n",
    "# Drop the UniqueID column and remove duplicates\n",
    "water_really_clean = water_combined.drop(columns='UniqueID').drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSeg2003 True\n",
      "CBSeg2003Description True\n",
      "Station True\n",
      "Latitude True\n",
      "Longitude True\n",
      "EventId True\n",
      "Source True\n",
      "SampleDepth True\n",
      "SampleDate True\n"
     ]
    }
   ],
   "source": [
    "# Check we haven't lost unique non-empty values\n",
    "for col in columns_to_group:\n",
    "    # Filter out empty values\n",
    "    df_combined_nonempty = water_really_clean[col].dropna()\n",
    "    water_pivoted_nonempty = water[col].dropna()\n",
    "    \n",
    "    # Check if the number of unique non-empty values matches\n",
    "    unique_check = df_combined_nonempty.unique().size == water_pivoted_nonempty.unique().size\n",
    "    \n",
    "    # Print the results\n",
    "    print(col, unique_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_really_clean.to_csv('../data/plank_ChesapeakeBayBenthicWater_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Dataset\n",
    "\n",
    "Now we combine on 'CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
    "       'EventId', 'Source', 'SampleDate', 'SampleDepth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sediment columns: Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'EventId', 'TotalDepth', 'SampleReplicate', 'SampleDate',\n",
      "       'ReportingParameter', 'ReportedValue'],\n",
      "      dtype='object')\n",
      "BioMass columns: Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'FieldActivityId', 'BiologicalEventId', 'Source', 'SampleDate',\n",
      "       'TotalDepth', 'SampleTime', 'SampleReplicate', 'IBIParameter',\n",
      "       'IBIValue'],\n",
      "      dtype='object')\n",
      "Water columns Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'EventId', 'Source', 'SampleDate', 'SampleDepth', 'ReportedParameter',\n",
      "       'ReportedValue'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Sediment columns:', sediment_clean.columns)\n",
    "print('BioMass columns:', biomass_clean.columns)\n",
    "print('Water columns', water_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
