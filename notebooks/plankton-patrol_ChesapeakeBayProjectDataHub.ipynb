{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "The Chesapeake Bay Program [DataHub](https://datahub.chesapeakebay.net/Home) contains many datasets for the Chesapeake Bay. The Water Quality Data is still updating and measures many field and lab parameters including: phosphorus, nitrogen, carbon, various other lab parameters (suspended solids, disolved solids, chlorophyll-a, alkalinkity, etc), dissolved oxygen, pH, salinity, turbitity, water temperature, and climate condition. \n",
    "\n",
    "See [Guide to Using Chesapeake Bay Program Water Quality Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/wq_data_userguide_10feb12_mod.pdf) for more information.\n",
    " \n",
    "There is also a [DataHub API](https://datahub.chesapeakebay.net/API) which we will use to access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What data do we want?\n",
    "\n",
    "The Chesapeake Bay segements are based on circulation and salinity properties of different areas of the Bay. We want the segements in the Chesapeake Bay proper, which haveCBSeg2003Name that starts \"CB\".\n",
    "[Map of the segments](https://www.chesapeakebay.net/what/maps/chesapeake-bay-2003-segmentation-scheme-codes).\n",
    "\n",
    "The Water Quality data contains quite a lot of data, so we will define functions to download 5 years of data at once. This method will also work for other large datasets, although the plankton databases seem to download over long time periods without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def generate_date_ranges(start_date, end_date, delta_years=5):\n",
    "    # Generate date ranges of specified years between start_date and end_date.\n",
    "    date_list =[]\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + relativedelta(years=delta_years)\n",
    "        date_list.append(current_date.strftime('%m-%d-%Y') +'/' \n",
    "                         + next_date.strftime('%m-%d-%Y') +'/') \n",
    "        current_date = next_date\n",
    "    return(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def fetch_and_save_data_by_date(base_url, url_idValues, start_date, end_date, output_file):\n",
    "    # Generate list of date ranges\n",
    "    date_list = generate_date_ranges(start_date, end_date)\n",
    "    \n",
    "    # Open the output file in append mode\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = None\n",
    "        \n",
    "        for dates in date_list:\n",
    "            \n",
    "            # Define the API endpoint URL with CSV format\n",
    "            api_url = base_url + dates + url_idValues\n",
    "            # Send a GET request to the API endpoint\n",
    "            response = requests.get(api_url)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Read the CSV response\n",
    "                csv_data = response.text.splitlines()\n",
    "                csv_reader = csv.reader(csv_data)\n",
    "                \n",
    "                # Write the CSV data to the file\n",
    "                if csv_writer is None:\n",
    "                    # Write the header only once\n",
    "                    csv_writer = csv.writer(csvfile)\n",
    "                    csv_writer.writerow(next(csv_reader))  # Write header\n",
    "                \n",
    "                for row in csv_reader:\n",
    "                    csv_writer.writerow(row)\n",
    "                \n",
    "                print(f\"Data from {dates} saved to {output_file}\")\n",
    "            else:\n",
    "                # Handle the error\n",
    "                print(f\"Failed to retrieve data from {dates}: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Water Quality\n",
    "\n",
    "Water Quality `http://datahub.chesapeakebay.net/api.{format}/WaterQuality/<Start-Date>/<End-Date>/<Data-Stream-Value>/<Program-Id>/<Project-Id>/<Geographical-Attribute>/<Attribute-Id>/<Substance-Id>`\n",
    "\n",
    "(Data-Stream-Value list)[https://datahub.chesapeakebay.net/api.json/DataStreams] We want all data `0,1`\n",
    "\n",
    "(Program-Id list for water quality)[https://datahub.chesapeakebay.net/api.json/WaterQuality/Programs] We want all three: `2,4,6`\n",
    "\n",
    "Now the base url is `https://datahub.chesapeakebay.net/api.CSV/WaterQuality/WaterQuality/7-29-2014/7-29-2024/0,1/2,4,6/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function to call the API for our entire desired time frame and output the data in one CSV. Note that for each database, we will need a `base_url` which points to the desired database and output format, a `url_idValues` which tells the API which values to download (the length of this url also depends on the database), and an `output_file`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Water Quality Data\n",
    "\n",
    "Water Quality `http://datahub.chesapeakebay.net/api.{format}/WaterQuality/<Start-Date>/<End-Date>/<Data-Stream-Value>/<Program-Id>/<Project-Id>/<Geographical-Attribute>/<Attribute-Id>/<Substance-Id>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base URL\n",
    "base_url = 'https://datahub.chesapeakebay.net/api.CSV/WaterQuality/WaterQuality/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(Data-Stream-Value list)[https://datahub.chesapeakebay.net/api.json/DataStreams] We want all data `0,1`\n",
    "\n",
    "(Program-Id list for water quality)[https://datahub.chesapeakebay.net/api.json/WaterQuality/Programs] We want all three: `2,4,6`\n",
    "\n",
    "We can update our url to `https://datahub.chesapeakebay.net/api.CSV/WaterQuality/WaterQuality/<Start-Date>/<End-Date>/0,1/2,4,6/<Project-Id>/<Geographical-Attribute>/<Attribute-Id>/<Substance-Id>`. We will deal with the date last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the id values that go after <Start-Date>/<End-Date>\n",
    "url_idValues = '0,1/2,4,6/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all project ids \n",
    "(Project-ID list for water quality)[https://datahub.chesapeakebay.net/api.json/WaterQuality/Projects]\n",
    "Note that some projects might not have any water quality data for the desired segments, but that does not seem to cause a problem with the API timing out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1/2,4,6/12,13,14,15,35,36,2,3,11,7,33,34,23,24,16/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL with the Projects list\n",
    "projectList_url = \"https://datahub.chesapeakebay.net/api.json/WaterQuality/Projects\"\n",
    "\n",
    "# Send a GET request to the list\n",
    "response = requests.get(projectList_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    # Find ProjectId\n",
    "    projectIds = [segment['ProjectId'] for segment in data]\n",
    "    \n",
    "    # Append ids to idValues\n",
    "    for projectId in projectIds:\n",
    "        url_idValues = url_idValues + str(projectId) +','\n",
    "\n",
    "    # Remove final comma, append /\n",
    "    url_idValues = url_idValues[:-1] + '/'\n",
    "    print(url_idValues)\n",
    "else:\n",
    "    # Handle the error\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to retrieve the `Geographical-Id`s from the (Geographical-Attribute, CBSeg2003 list)[https://datahub.chesapeakebay.net/api.json/CBSeg2003]. Since we only want the segments in the Bay proper, we will search for the segment names that start with `CB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1/2,4,6/12,13,14,15,35,36,2,3,11,7,33,34,23,24,16/CBSeg2003/10,11,12,13,14,15,16,17/\n"
     ]
    }
   ],
   "source": [
    "# Add CBSeg2003 to idValues to specify type og geographic id\n",
    "url_idValues = url_idValues +'CBSeg2003/'\n",
    "\n",
    "# Define the URL with the CBSeg2003 list\n",
    "CBSeg2003_url = \"http://datahub.chesapeakebay.net/api.json/CBSeg2003\"\n",
    "\n",
    "# Send a GET request to the list\n",
    "response = requests.get(CBSeg2003_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    # Filter the results to find CBSeg2003Name that start with \"CB\"\n",
    "    filtered_segments = [segment for segment in data if segment['CBSeg2003Name'].startswith('CB')]\n",
    "    # Extract the CBSeg2003Id for the filtered results\n",
    "    segmentIds = [segment['CBSeg2003Id'] for segment in filtered_segments]\n",
    "\n",
    "    # Append ids to idValues\n",
    "    for segmentId in segmentIds:\n",
    "        url_idValues = url_idValues + str(segmentId) +','\n",
    "\n",
    "    # Remove final comma, append /\n",
    "    url_idValues = url_idValues[:-1] + '/'\n",
    "    print(url_idValues)\n",
    "else:\n",
    "    # Handle the error\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to append the `Substance-Id`. Now, there are many substances that are not measured in the dataset and regions we want. Using the online download form, it looks like the largest possible list is `21,30,31,35,36,49,55,60,63,65,67,71,73,74,77,78,82,83,85,87,88,94,104,105,109,111,114,116,121,123,33,76,113,34,119`. I don't see a more systematic way to do this step, since different stations are measuring different things.\n",
    "\n",
    "In addition to updating the `url_idValues`, let's also pull a dictionary for these substances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SubstanceId': 21, 'SubstanceIdentificationName': 'CHLA', 'SubstanceIdentificationDescription': 'Active Chlorophyll-A'}\n",
      "{'SubstanceId': 30, 'SubstanceIdentificationName': 'DIN', 'SubstanceIdentificationDescription': 'Dissolved Inorganic Nitrogen'}\n",
      "{'SubstanceId': 31, 'SubstanceIdentificationName': 'DO', 'SubstanceIdentificationDescription': 'Dissolved Oxygen In MG/L'}\n",
      "{'SubstanceId': 33, 'SubstanceIdentificationName': 'DO_SAT_P', 'SubstanceIdentificationDescription': 'DO Saturation Using Probe Units In Percent'}\n",
      "{'SubstanceId': 34, 'SubstanceIdentificationName': 'DOC', 'SubstanceIdentificationDescription': 'Dissolved Organic Carbon'}\n",
      "{'SubstanceId': 35, 'SubstanceIdentificationName': 'DON', 'SubstanceIdentificationDescription': 'Dissolved Organic Nitrogen'}\n",
      "{'SubstanceId': 36, 'SubstanceIdentificationName': 'DOP', 'SubstanceIdentificationDescription': 'Dissolved Organic Phosphorus'}\n",
      "{'SubstanceId': 49, 'SubstanceIdentificationName': 'FSS', 'SubstanceIdentificationDescription': 'Fixed Suspended Solids'}\n",
      "{'SubstanceId': 55, 'SubstanceIdentificationName': 'KD', 'SubstanceIdentificationDescription': 'Light Attenuation'}\n",
      "{'SubstanceId': 60, 'SubstanceIdentificationName': 'NH4F', 'SubstanceIdentificationDescription': 'Ammonium Nitrogen As N (Filtered Sample)'}\n",
      "{'SubstanceId': 63, 'SubstanceIdentificationName': 'NO23F', 'SubstanceIdentificationDescription': 'Nitrite+Nitrate Nitrogen As N (Filtered Sample)'}\n",
      "{'SubstanceId': 65, 'SubstanceIdentificationName': 'NO2F', 'SubstanceIdentificationDescription': 'Nitrite Nitrogen As N (Filtered Sample)'}\n",
      "{'SubstanceId': 67, 'SubstanceIdentificationName': 'NO3F', 'SubstanceIdentificationDescription': 'Nitrate Nitrogen As N (Filtered Sample)'}\n",
      "{'SubstanceId': 71, 'SubstanceIdentificationName': 'PC', 'SubstanceIdentificationDescription': 'Particulate Carbon; Inorganic + Organic'}\n",
      "{'SubstanceId': 73, 'SubstanceIdentificationName': 'PH', 'SubstanceIdentificationDescription': 'Ph Corrected For Temperature (25 Deg C)'}\n",
      "{'SubstanceId': 74, 'SubstanceIdentificationName': 'PHEO', 'SubstanceIdentificationDescription': 'Pheophytin'}\n",
      "{'SubstanceId': 76, 'SubstanceIdentificationName': 'PIP', 'SubstanceIdentificationDescription': 'Particulate Inorganic Phosphorus'}\n",
      "{'SubstanceId': 77, 'SubstanceIdentificationName': 'PN', 'SubstanceIdentificationDescription': 'Particulate Nitrogen'}\n",
      "{'SubstanceId': 78, 'SubstanceIdentificationName': 'PO4F', 'SubstanceIdentificationDescription': 'Orthophosphate Phosphorus As P (Filtered Sample)'}\n",
      "{'SubstanceId': 82, 'SubstanceIdentificationName': 'PP', 'SubstanceIdentificationDescription': 'Particulate Phosphorus'}\n",
      "{'SubstanceId': 83, 'SubstanceIdentificationName': 'SALINITY', 'SubstanceIdentificationDescription': 'Salinity  Units Are Parts Per Thousand (Ppt) And Are Equal To Practical Salnity Units (Psu).'}\n",
      "{'SubstanceId': 85, 'SubstanceIdentificationName': 'SECCHI', 'SubstanceIdentificationDescription': 'Secchi Depth'}\n",
      "{'SubstanceId': 87, 'SubstanceIdentificationName': 'SIF', 'SubstanceIdentificationDescription': 'Silica As Si (Filtered Sample)'}\n",
      "{'SubstanceId': 88, 'SubstanceIdentificationName': 'SIGMA_T', 'SubstanceIdentificationDescription': 'Water Density; Dependent On Salinity And Wtemp'}\n",
      "{'SubstanceId': 94, 'SubstanceIdentificationName': 'SPCOND', 'SubstanceIdentificationDescription': 'Conductivity Corrected For Temperature (25 Deg C) And Salinity'}\n",
      "{'SubstanceId': 104, 'SubstanceIdentificationName': 'TDN', 'SubstanceIdentificationDescription': 'Total Dissolved Nitrogen'}\n",
      "{'SubstanceId': 105, 'SubstanceIdentificationName': 'TDP', 'SubstanceIdentificationDescription': 'Total Dissolved Phosphorus'}\n",
      "{'SubstanceId': 109, 'SubstanceIdentificationName': 'TN', 'SubstanceIdentificationDescription': 'Total Nitrogen'}\n",
      "{'SubstanceId': 111, 'SubstanceIdentificationName': 'TON', 'SubstanceIdentificationDescription': 'Total Organic Nitrogen'}\n",
      "{'SubstanceId': 113, 'SubstanceIdentificationName': 'TOTAL_DEPTH', 'SubstanceIdentificationDescription': 'Total Station Depth'}\n",
      "{'SubstanceId': 114, 'SubstanceIdentificationName': 'TP', 'SubstanceIdentificationDescription': 'Total Phosphorus'}\n",
      "{'SubstanceId': 116, 'SubstanceIdentificationName': 'TSS', 'SubstanceIdentificationDescription': 'Total Suspended Solids'}\n",
      "{'SubstanceId': 119, 'SubstanceIdentificationName': 'TURB_NTU', 'SubstanceIdentificationDescription': 'Turbidity; Nephelometric Method'}\n",
      "{'SubstanceId': 121, 'SubstanceIdentificationName': 'VSS', 'SubstanceIdentificationDescription': 'Volatile Suspended Solids'}\n",
      "{'SubstanceId': 123, 'SubstanceIdentificationName': 'WTEMP', 'SubstanceIdentificationDescription': 'Water Temperature'}\n"
     ]
    }
   ],
   "source": [
    "substanceId_list = [21,30,31,33,34,35,36,49,55,60,63,65,67,71,73,74,76,77,78,82,83,85,87,88,94,104,105,109,111,113,114,116,119,121,123]\n",
    "\n",
    "\n",
    "# Define the URL with the substance list\n",
    "substanceId_url = \"https://datahub.chesapeakebay.net/api.json/Substances\"\n",
    "\n",
    "# Send a GET request to the list\n",
    "response = requests.get(substanceId_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    # Filter the results to find substances with SubstanceId in substanceId_list\n",
    "    filtered_substances = [substance for substance in data if substance['SubstanceId'] in substanceId_list]\n",
    "\n",
    "    # Print the filtered substances\n",
    "    for substance in filtered_substances:\n",
    "        print(substance)\n",
    "else:\n",
    "    # Handle the error\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_idValues = url_idValues +'21,30,31,33,34,35,36,49,55,60,63,65,67,71,73,74,76,77,78,82,83,85,87,88,94,104,105,109,111,113,114,116,119,121,123'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And call the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from 07-29-2004/07-29-2009/ saved to ../data/plankton-patrol_ChesapeakeWaterQuality.csv\n",
      "Data from 07-29-2009/07-29-2014/ saved to ../data/plankton-patrol_ChesapeakeWaterQuality.csv\n",
      "Data from 07-29-2014/07-29-2019/ saved to ../data/plankton-patrol_ChesapeakeWaterQuality.csv\n",
      "Data from 07-29-2019/07-29-2024/ saved to ../data/plankton-patrol_ChesapeakeWaterQuality.csv\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime(2004, 7, 29)\n",
    "end_date = datetime(2024, 7, 29)\n",
    "output_file = '../data/plankton-patrol_ChesapeakeWaterQuality.csv'\n",
    "\n",
    "fetch_and_save_data_by_date(base_url, url_idValues, start_date, end_date, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data with help of VSCode Data Wrangle extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rl/kqt6tbv90l9_pwc4927vdb340000gn/T/ipykernel_36178/3805926513.py:10: DtypeWarning: Columns (1,10,11,12,13,19,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r'/Users/clairemerriman/Documents/GitHub/na-erdos-fellows-monorepo/data/plankton-patrol_ChesapeakeWaterQuality.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSeg2003</th>\n",
       "      <th>EventId</th>\n",
       "      <th>Cruise</th>\n",
       "      <th>Program</th>\n",
       "      <th>Project</th>\n",
       "      <th>Agency</th>\n",
       "      <th>Source</th>\n",
       "      <th>Station</th>\n",
       "      <th>SampleDate</th>\n",
       "      <th>SampleTime</th>\n",
       "      <th>...</th>\n",
       "      <th>SampleReplicateType</th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Qualifier</th>\n",
       "      <th>MeasureValue</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Method</th>\n",
       "      <th>Lab</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>TierLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>88268</td>\n",
       "      <td>BAY475</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB2.1</td>\n",
       "      <td>2/19/2008</td>\n",
       "      <td>10:18:00</td>\n",
       "      <td>...</td>\n",
       "      <td>S1</td>\n",
       "      <td>CHLA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.495</td>\n",
       "      <td>UG/L</td>\n",
       "      <td>L01</td>\n",
       "      <td>MDHMH</td>\n",
       "      <td>39.44149</td>\n",
       "      <td>-76.02599</td>\n",
       "      <td>T3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>88268</td>\n",
       "      <td>BAY475</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB2.1</td>\n",
       "      <td>2/19/2008</td>\n",
       "      <td>10:18:00</td>\n",
       "      <td>...</td>\n",
       "      <td>S1</td>\n",
       "      <td>CHLA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UG/L</td>\n",
       "      <td>L01</td>\n",
       "      <td>MDHMH</td>\n",
       "      <td>39.44149</td>\n",
       "      <td>-76.02599</td>\n",
       "      <td>T3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>130482</td>\n",
       "      <td>BAY477</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB2.1</td>\n",
       "      <td>3/21/2008</td>\n",
       "      <td>12:33:00</td>\n",
       "      <td>...</td>\n",
       "      <td>S1</td>\n",
       "      <td>CHLA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UG/L</td>\n",
       "      <td>L01</td>\n",
       "      <td>MDHMH</td>\n",
       "      <td>39.44149</td>\n",
       "      <td>-76.02599</td>\n",
       "      <td>T3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>130482</td>\n",
       "      <td>BAY477</td>\n",
       "      <td>TWQM</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>CB2.1</td>\n",
       "      <td>3/21/2008</td>\n",
       "      <td>12:33:00</td>\n",
       "      <td>...</td>\n",
       "      <td>S1</td>\n",
       "      <td>CHLA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.233</td>\n",
       "      <td>UG/L</td>\n",
       "      <td>L01</td>\n",
       "      <td>MDHMH</td>\n",
       "      <td>39.44149</td>\n",
       "      <td>-76.02599</td>\n",
       "      <td>T3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CB1TF</td>\n",
       "      <td>130623</td>\n",
       "      <td>BAY478</td>\n",
       "      <td>SWM</td>\n",
       "      <td>DFLO</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>MDDNR</td>\n",
       "      <td>XJH8658</td>\n",
       "      <td>4/3/2008</td>\n",
       "      <td>08:51:00</td>\n",
       "      <td>...</td>\n",
       "      <td>S1</td>\n",
       "      <td>CHLA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.495</td>\n",
       "      <td>UG/L</td>\n",
       "      <td>L01</td>\n",
       "      <td>MDHMH</td>\n",
       "      <td>39.4767</td>\n",
       "      <td>-76.0709</td>\n",
       "      <td>T3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CBSeg2003 EventId  Cruise Program Project Agency Source  Station SampleDate  \\\n",
       "0    CB1TF    88268  BAY475    TWQM    MAIN  MDDNR  MDDNR    CB2.1  2/19/2008   \n",
       "1    CB1TF    88268  BAY475    TWQM    MAIN  MDDNR  MDDNR    CB2.1  2/19/2008   \n",
       "2    CB1TF   130482  BAY477    TWQM    MAIN  MDDNR  MDDNR    CB2.1  3/21/2008   \n",
       "3    CB1TF   130482  BAY477    TWQM    MAIN  MDDNR  MDDNR    CB2.1  3/21/2008   \n",
       "4    CB1TF   130623  BAY478     SWM    DFLO  MDDNR  MDDNR  XJH8658   4/3/2008   \n",
       "\n",
       "  SampleTime  ... SampleReplicateType Parameter Qualifier MeasureValue  Unit  \\\n",
       "0   10:18:00  ...                  S1      CHLA       NaN        1.495  UG/L   \n",
       "1   10:18:00  ...                  S1      CHLA       NaN          NaN  UG/L   \n",
       "2   12:33:00  ...                  S1      CHLA       NaN          NaN  UG/L   \n",
       "3   12:33:00  ...                  S1      CHLA       NaN        5.233  UG/L   \n",
       "4   08:51:00  ...                  S1      CHLA       NaN        1.495  UG/L   \n",
       "\n",
       "  Method    Lab  Latitude Longitude TierLevel  \n",
       "0   L01   MDHMH  39.44149 -76.02599        T3  \n",
       "1   L01   MDHMH  39.44149 -76.02599        T3  \n",
       "2   L01   MDHMH  39.44149 -76.02599        T3  \n",
       "3   L01   MDHMH  39.44149 -76.02599        T3  \n",
       "4   L01   MDHMH   39.4767  -76.0709        T3  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_data(df):\n",
    "    # Drop empty columns: 'PrecisionPC', 'BiasPC'\n",
    "    # Drop columns that are almost all empty or nan\n",
    "    df = df.drop(columns=['PrecisionPC','BiasPC','Details','Problem'])\n",
    "    return df\n",
    "\n",
    "# Loaded variable 'df' from URI: /Users/clairemerriman/Documents/GitHub/na-erdos-fellows-monorepo/data/plankton-patrol_ChesapeakeWaterQuality.csv\n",
    "df = pd.read_csv(r'/Users/clairemerriman/Documents/GitHub/na-erdos-fellows-monorepo/data/plankton-patrol_ChesapeakeWaterQuality.csv')\n",
    "\n",
    "df_clean = clean_data(df.copy())\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioMass\n",
    "\n",
    "There are many different parts of the Living Resources database. Let's start by looking at BioMass, which is part of the `TidalBenthic` dataset. The url format is `http://datahub.chesapeakebay.net/api.{format}/LivingResources/TidalBenthic/BioMass/<Start-Date>/<End-Date>/<Project-Id>/<Geographical-Attribute>/<Attribute-Id>`. It appears the `Attribut-Id` is optional.\n",
    "\n",
    "Note that the API does not allow combining projects like we did in the Water Quality dataset, but it can download the entire timeframe without timeout errors. We define a new function to retrieve the data. This function can be used for any of the Living Resources datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def fetch_and_save_data_by_project(base_url, url_idValues, start_date, end_date, project_list, output_file):\n",
    "    # Format the dates\n",
    "    start_str = start_date.strftime('%m-%d-%Y')\n",
    "    end_str = end_date.strftime('%m-%d-%Y')\n",
    "    \n",
    "    # Open the output file in append mode\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = None\n",
    "        \n",
    "        for project in project_list:\n",
    "            \n",
    "            # Define the API endpoint URL with CSV format\n",
    "            api_url = f\"{base_url}{start_str}/{end_str}/{project[1]}/{url_idValues}\"\n",
    "            # Send a GET request to the API endpoint\n",
    "            response = requests.get(api_url)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Read the CSV response\n",
    "                csv_data = response.text.splitlines()\n",
    "                csv_reader = csv.reader(csv_data)\n",
    "                \n",
    "                # Write the CSV data to the file\n",
    "                if csv_writer is None:\n",
    "                    # Write the header only once\n",
    "                    csv_writer = csv.writer(csvfile)\n",
    "                    csv_writer.writerow(next(csv_reader))  # Write header\n",
    "                \n",
    "                for row in csv_reader:\n",
    "                    csv_writer.writerow(row)\n",
    "                \n",
    "                print(f\"Data from {project[0]} saved to {output_file}\")\n",
    "            else:\n",
    "                # Handle the error\n",
    "                print(f\"Failed to retrieve data from {dates}: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We define a new `base_url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://datahub.chesapeakebay.net/api.csv/LivingResources/TidalBenthic/BioMass/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the project list. From the online download menu, the relevent projects are BEN-Tidal Benthic Monitoring and SBEN-Special Tidal Benthic Monitoring. We may also want to add CBEN-Coastal Bays Benthic Monitoring, which are bays adjacent to the Chesapeake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tidal Benthic Monitoring', 1], ['Special Tidal Benthic Monitoring', 32]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize url_idValues\n",
    "url_idValues = \"\"\n",
    "\n",
    "projectIdentifier_list = ['BEN','SBEN']\n",
    "\n",
    "\n",
    "# Define the URL with the substance list\n",
    "projectIdentifier_url = \"https://datahub.chesapeakebay.net/api.json/LivingResources/Projects\"\n",
    "\n",
    "# Send a GET request to the list\n",
    "response = requests.get(projectIdentifier_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    # Filter the results to find projects with ProjectIdentifier in projectIdentifier_list\n",
    "    filtered_projects = [project for project in data if project['ProjectIdentifier'] in projectIdentifier_list]\n",
    "\n",
    "    # Extract the ProjectID for the filtered results\n",
    "    projectIds = [[project['ProjectName'],project['ProjectId']] for project in filtered_projects]\n",
    "\n",
    "\n",
    "    print(projectIds)\n",
    "else:\n",
    "    # Handle the error\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Geographic-Id` is the same as for Water Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSeg2003/10,11,12,13,14,15,16,17/\n"
     ]
    }
   ],
   "source": [
    "# Add CBSeg2003 to idValues to specify type og geographic id\n",
    "url_idValues = url_idValues +'CBSeg2003/'\n",
    "\n",
    "# Define the URL with the CBSeg2003 list\n",
    "CBSeg2003_url = \"http://datahub.chesapeakebay.net/api.json/CBSeg2003\"\n",
    "\n",
    "# Send a GET request to the list\n",
    "response = requests.get(CBSeg2003_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    # Filter the results to find CBSeg2003Name that start with \"CB\"\n",
    "    filtered_segments = [segment for segment in data if segment['CBSeg2003Name'].startswith('CB')]\n",
    "    # Extract the CBSeg2003Id for the filtered results\n",
    "    segmentIds = [segment['CBSeg2003Id'] for segment in filtered_segments]\n",
    "\n",
    "    # Append ids to idValues\n",
    "    for segmentId in segmentIds:\n",
    "        url_idValues = url_idValues + str(segmentId) +','\n",
    "\n",
    "    # Remove final comma, append /\n",
    "    url_idValues = url_idValues[:-1] + '/'\n",
    "    print(url_idValues)\n",
    "else:\n",
    "    # Handle the error\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need any additional information, so let's run the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from Tidal Benthic Monitoring saved to ../data/plankton-patrol_ChesapeakeBioMass.csv\n",
      "Data from Special Tidal Benthic Monitoring saved to ../data/plankton-patrol_ChesapeakeBioMass.csv\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime(2004, 7, 29)\n",
    "end_date = datetime(2024, 7, 29)\n",
    "output_file = '../data/plankton-patrol_ChesapeakeBioMass.csv'\n",
    "\n",
    "fetch_and_save_data_by_project(base_url, url_idValues, start_date, end_date, projectIds,output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up with VSCode Data Wrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSeg2003</th>\n",
       "      <th>CBSeg2003Description</th>\n",
       "      <th>FieldActivityId</th>\n",
       "      <th>BiologicalEventId</th>\n",
       "      <th>Source</th>\n",
       "      <th>SampleDate</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Station</th>\n",
       "      <th>TotalDepth</th>\n",
       "      <th>SampleTime</th>\n",
       "      <th>SampleReplicate</th>\n",
       "      <th>IBIParameter</th>\n",
       "      <th>IBIValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CB5MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>215580</td>\n",
       "      <td>68852</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>8/31/2004</td>\n",
       "      <td>38.30810</td>\n",
       "      <td>-76.37270</td>\n",
       "      <td>11513</td>\n",
       "      <td>8.1</td>\n",
       "      <td>06:58:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>PCT_CARN_OMN</td>\n",
       "      <td>21.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CB5MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>215580</td>\n",
       "      <td>68852</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>8/31/2004</td>\n",
       "      <td>38.30810</td>\n",
       "      <td>-76.37270</td>\n",
       "      <td>11513</td>\n",
       "      <td>8.1</td>\n",
       "      <td>06:58:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>PCT_DEPO</td>\n",
       "      <td>15.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CB5MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>215580</td>\n",
       "      <td>68852</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>8/31/2004</td>\n",
       "      <td>38.30810</td>\n",
       "      <td>-76.37270</td>\n",
       "      <td>11513</td>\n",
       "      <td>8.1</td>\n",
       "      <td>06:58:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>PCT_PI_ABUND</td>\n",
       "      <td>11.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CB5MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>215580</td>\n",
       "      <td>68852</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>8/31/2004</td>\n",
       "      <td>38.30810</td>\n",
       "      <td>-76.37270</td>\n",
       "      <td>11513</td>\n",
       "      <td>8.1</td>\n",
       "      <td>06:58:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>PCT_PI_BIO</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CB5MH</td>\n",
       "      <td>Chesapeake Bay-Mesohaline Region</td>\n",
       "      <td>215580</td>\n",
       "      <td>68852</td>\n",
       "      <td>VERSAR/EME/BEL</td>\n",
       "      <td>8/31/2004</td>\n",
       "      <td>38.30810</td>\n",
       "      <td>-76.37270</td>\n",
       "      <td>11513</td>\n",
       "      <td>8.1</td>\n",
       "      <td>06:58:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>PCT_PS_ABUND</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CBSeg2003              CBSeg2003Description FieldActivityId  \\\n",
       "0    CB5MH   Chesapeake Bay-Mesohaline Region          215580   \n",
       "1    CB5MH   Chesapeake Bay-Mesohaline Region          215580   \n",
       "2    CB5MH   Chesapeake Bay-Mesohaline Region          215580   \n",
       "3    CB5MH   Chesapeake Bay-Mesohaline Region          215580   \n",
       "4    CB5MH   Chesapeake Bay-Mesohaline Region          215580   \n",
       "\n",
       "  BiologicalEventId          Source SampleDate  Latitude  Longitude Station  \\\n",
       "0             68852  VERSAR/EME/BEL  8/31/2004  38.30810  -76.37270   11513   \n",
       "1             68852  VERSAR/EME/BEL  8/31/2004  38.30810  -76.37270   11513   \n",
       "2             68852  VERSAR/EME/BEL  8/31/2004  38.30810  -76.37270   11513   \n",
       "3             68852  VERSAR/EME/BEL  8/31/2004  38.30810  -76.37270   11513   \n",
       "4             68852  VERSAR/EME/BEL  8/31/2004  38.30810  -76.37270   11513   \n",
       "\n",
       "  TotalDepth SampleTime SampleReplicate  IBIParameter IBIValue  \n",
       "0        8.1   06:58:00              S1  PCT_CARN_OMN    21.57  \n",
       "1        8.1   06:58:00              S1      PCT_DEPO    15.69  \n",
       "2        8.1   06:58:00              S1  PCT_PI_ABUND    11.76  \n",
       "3        8.1   06:58:00              S1    PCT_PI_BIO     4.62  \n",
       "4        8.1   06:58:00              S1  PCT_PS_ABUND      9.8  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_data(df):\n",
    "    # Drop column: 'SiteType' with only one value\n",
    "    df = df.drop(columns=['SiteType'])\n",
    "    return df\n",
    "\n",
    "# Loaded variable 'df' from URI: /Users/clairemerriman/Documents/GitHub/na-erdos-fellows-monorepo/data/plankton-patrol_ChesapeakeBioMass.csv\n",
    "df = pd.read_csv(r'/Users/clairemerriman/Documents/GitHub/na-erdos-fellows-monorepo/data/plankton-patrol_ChesapeakeBioMass.csv')\n",
    "\n",
    "df_clean = clean_data(df.copy())\n",
    "df_clean.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
